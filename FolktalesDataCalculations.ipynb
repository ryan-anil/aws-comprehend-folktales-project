{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e154b75",
   "metadata": {},
   "source": [
    "### Comprehend NER\n",
    "\n",
    "This notebook goes over the steps for using AWS Comprehend's Detect Entities tool. This method performs Named Entity Recognition over a text or collection of texts and tags terms or phrases which are indicative of a specific named object, thing, person, or any other type of entity. In order to use Comprehend in Python, you must use the boto3 SDK and create a Comprehend client. We use this client to request NLP results and create maniputable dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf6b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "comprehend = boto3.client('comprehend')\n",
    "\n",
    "buckets = ['arabian-folktales-transformed-comprehendable','chinese-folktales-transformed-comprehendable','english-folktales-transformed-comprehendable','german-folktales-transformed-comprehendable','indian-folktales-transformed-comprehendable','russian-folktales-transformed-comprehendable']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37462d37",
   "metadata": {},
   "source": [
    "In our project we will analyze the results of Comprehend's NER annotator. Our input files are cleaned text files which are less than 5,000 bytes in size. In order to maintain consistency in results, we will group the stories by culture of origin and create dataframes for each section containing the pertinent information for each word tagged by the NER annotator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "191fa31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.create_bucket(Bucket = 'folktales-comprehend-ner')\n",
    "\n",
    "sections = ['arabian','chinese','english','german','indian','russian']\n",
    "count = 0;\n",
    "for currBucket in buckets:\n",
    "    scoreList = []\n",
    "    typeList = []\n",
    "    tokenList = []\n",
    "    beginList = []\n",
    "    endList = []\n",
    "    for item in s3.list_objects_v2(Bucket = currBucket).get('Contents'):\n",
    "        resultsNER = comprehend.detect_entities(Text = s3.get_object(Bucket = currBucket, Key = item.get('Key')).get('Body').read().decode('utf-8'), LanguageCode = 'en')\n",
    "        for entity in resultsNER.get('Entities'):\n",
    "            scoreList.append(entity.get('Score'))\n",
    "            typeList.append(entity.get('Type'))\n",
    "            tokenList.append(entity.get('Text'))\n",
    "            beginList.append(entity.get('BeginOffset'))\n",
    "            endList.append(entity.get('EndOffset'))\n",
    "        \n",
    "    df = pd.DataFrame({'Score': scoreList, 'Type': typeList, 'Text': tokenList, 'BeginOffset': beginList, 'EndOffset': endList})\n",
    "    \n",
    "    key = 'ner-results-' + sections[count] + '.csv'\n",
    "    count += 1\n",
    "    \n",
    "    s3.put_object(Bucket = 'folktales-comprehend-ner', Body = df.to_csv(), Key = key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5e745",
   "metadata": {},
   "source": [
    "Comprehend gives us two available methods for requesting NER annotations: single documents or with batches up to 25 documents, all less than 5,000 bytes. Because some of our 'transformed-comprehendable' buckets contain more than 25 documents, we request NER annotation for each document available to us, one at a time. For each document, we gather the data for each tagged entity, primarily caring for the results in the 'Score','Type', and 'Text' columns. We create a dataframe for each section and then move these csv files to a new S3 bucket."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
